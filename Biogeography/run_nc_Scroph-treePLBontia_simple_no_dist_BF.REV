###################################################################################
# Bayesian implementation of DEC in RevBayes (Landis et al. 2018)
#
# Script for the ancestral range reconstruction analysis performed in RevBayes 
# using DEC with a simple, non-stratified model (M0) with
# equal dispersal rate over time
#
# Author: Isabel Sanmartin (based on Michael Landis Tutorial

################################################################################
################################################################################
#!/usr/bin/rb

# filenames
range_fn = "data/areas.Scroph.18.05.2022.nex"
tree_fn  = "data/MCC_Scroph_treePL_Bontia_UCLN_burnin46.PRUNED.tre"
out_fn   = "output_simple_nc/Scroph_simple_nc_10000gen_simple-nodist_BF"
geo_fn   = "data/Scroph_nc"



# Read binary (01) presence-absence range data
dat_range_01 = readDiscreteCharacterData(range_fn)

# Convert binary ranges into NaturalNumbers
dat_range_n = formatDiscreteCharacterData(dat_range_01, "DEC")

# Check size of each vector
dat_range_n.size()
dat_range_01.size()

# Compare characters for two taxa
dat_range_01[1]
dat_range_n[1]

# Check data dimensions
n_areas  = dat_range_01.nchar()
n_states = floor(2^n_areas)

# There are 128 ranges or states for 7 areas (2exp(7))
# We are going to reduce the size of the Q matrix by applying constraints (no species lives in more than 6 areas).
# NOT WORTHY. WE ONLY LOSE ONE STATE (127)

#max_areas <- 6
#n_states <- 0
#for (k in 0:max_areas) n_states += choose(n_areas,k)

# Now number of states (n_states) is 120: equivalent to a sum of combinatorial elements (5 "chooses" 1 + 5 "chooses" 2 + 5 "chooses" 3 + 5 "chooses" 4)
# Notice that the "empty range" "0000" is a state in the DEC model!! Thus, we have 120 instead of 119 states
# Then use the new "n_states" to format the dataset for the reduced state space

dat_range_n = formatDiscreteCharacterData(dat_range_01, "DEC", n_states)

# Get the converted state descriptions
state_desc = dat_range_n.getStateDescriptions()

# Write the state descriptions to file. We will need that later.

state_desc_str = "state,range\n"
for (i in 1:state_desc.size())
{
    state_desc_str += (i-1) + "," + state_desc[i] + "\n"
}
write(state_desc_str, file=out_fn+".state_labels.txt")

# Create some move and monitor helper variables
mvi = 1
mni = 1
n_gen = 10000


###############
# Tree models #
###############

# read tree
tree <- readTrees(tree_fn)[1]


#######################
# Biogeography models #
#######################

## Set the biogeographic event rate multiplier. This assigns the migration rate baseline for the dispersal and extinction rate instantaneous Q matrix.
# In the tutorial example, this is set to be quite broad or uninformative. "range_bg" is set as a uniform distribution bounded 
# between 0.0001 (10exp(-4)) and 100 (10exp(2)), with an initial value of 0.01 (10exp(-2)).
# Here, we are going to use narrower, biologically more realistic priors. This is also important to avoid numerical overflow in estimating
# Bayes Factors by path sampling: as the sampler gets closer to the prior and further away from the posterior, where the data stops being informative,
# the combination of parameter values sampled from the prior becomes more "unlikely", which results in extremely low likelihoods even after
# log scaling, and eventually in numerical overflow. 
# To establish more realistic priors, we use the posterior estimates from an initial, test analysis with broader priors.

log10_rate_bg ~ dnUniform(-4,2)
log10_rate_bg.setValue(-2)
rate_bg := 10^log10_rate_bg
moves[mvi++] = mvSlide(log10_rate_bg, weight=4)


# Now, set the relative dispersal rate
dispersal_rate <- 1.0

####################### Dispersal model   #######################

# Build the relative dispersal rate matrix 
for (i in 1:n_areas) {
    for (j in 1:n_areas) {
        dr[i][j] <- dispersal_rate
    }
}



####################### Extirpation model ########################

# Set the relative extirpation rate (or per-area extinction rates)

# The default settings (Tutorial) set a lognormal with SD = 0.5, Mean = -0.125; Median= 0.882; 95% HPD: 0.388-2.01. 
# log_sd <- 0.5
# log_mean <- ln(1) - 0.5*log_sd^2

# This prior distribution is too narrow. Test trials with BF comparisons showed that we need a broader prior distribution. 
# We can use a prior that best approximate the posterior in trial runs, a broader prior.
# Tracer shows that this prior best fits the posterior: Mean = 1.5, Median = 1.2, 95% HPD= 0.29-3.43, value range = 0.21-7.17.
log_sd <- 0.5
log_mean <- ln(1) - 0.5*log_sd^2
extirpation_rate ~ dnLognormal(mean=log_mean, sd=log_sd)
moves[mvi++] = mvScale(extirpation_rate, weight=2)

# We set the extirpation rate as a lognormal with mean set as above

extirpation_rate ~ dnLognormal(mean=log_mean, sd=log_sd)
#moves.append( mvScale(extirpation_rate, weight=2) )
moves[mvi++] = mvScale(extirpation_rate, weight=2)


# Build the relative extirpation rate matrix

for (i in 1:n_areas) {
    for (j in 1:n_areas) {
        er[i][j] <- 0.0       
    }
    er[i][i] := extirpation_rate
}

####### Build the DEC anagenetic rate matrix #######

Q_DEC := fnDECRateMatrix(dispersalRates=dr,
                         extirpationRates=er)
                         #maxRangeSize=max_areas)


########### Cladogenetic DEC model ############

# Build cladogenetic transition probabilities. 
# Only sympatry (wide or peripatry, narrow) and allopatry (vicariance) are allowed.

clado_event_types <- [ "s", "a" ]
clado_event_probs <- simplex(1,1)

# We assign a simplex that sums to 1, but the proportion of the prior for each event is different.

#p_sympatry ~ dnUniform(0,1)
#p_allopatry := abs(1.0 - p_sympatry)
#clado_type_probs := simplex(p_sympatry, p_allopatry)
##moves.append( mvSlide(p_sympatry, weight=2) )
#moves[mvi++] = mvSlide(p_sympatry, weight=2)
#

# Build the DEC cladogenetic model
P_DEC := fnDECCladoProbs(eventProbs=clado_event_probs,
                         eventTypes=clado_event_types,
                         numCharacters=n_areas)
                         #maxRangeSize = max_areas)

# Construct the phylogenetic CTMC with cladogenetic events
m_bg ~ dnPhyloCTMCClado(tree=tree,
                           Q=Q_DEC,
                           cladoProbs=P_DEC,
                           branchRates=rate_bg,
                           type="NaturalNumbers",
                           nSites=1)
    
# attach the range data
m_bg.clamp(dat_range_n)

############
# Monitors #
############

monitors[mni++] =  mnScreen(printgen=10, rate_bg, extirpation_rate) 
monitors[mni++] =  mnModel(file=out_fn+".model.log", printgen=10) 
monitors[mni++] =  mnFile(tree, filename=out_fn+".tre", printgen=10) 
monitors[mni++] =  mnJointConditionalAncestralState(tree=tree,
                                                  ctmc=m_bg,
                                                  type="NaturalNumbers",
                                                  withTips=true,
                                                  withStartStates=true,
                                                  filename=out_fn+".states.log",
                                                  printgen=10) 
monitors[mni++] = mnStochasticCharacterMap(ctmc=m_bg,
                                          filename=out_fn+".stoch.log",
                                          printgen=10) 

############
# Analysis #
############


# Build the model analysis object from the model graph
mymodel = model(m_bg)

# Create the MCMC analysis object
mymcmc = mcmc(mymodel, monitors, moves)

# Run the MCMC analysis
mymcmc.run(n_gen)


#### Bayes Factor Comparison: Include code for path and stepping-stone sampling ####

### Compute power posterior distributions
# pow_p = powerPosterior(mymodel, moves, monitors, "outputPPsimple_nc/pow_p_DEC_simple_BF.out", cats=100, sampleFreq=10)
# pow_p.burnin(generations=10000,tuningInterval=100)
# pow_p.run(generations=1000)

### Use stepping-stone sampling to calculate marginal likelihoods

# ss = steppingStoneSampler(file="outputPPsimple_cp/pow_p_DEC_simple_BF.out", powerColumnName="power",
# likelihoodColumnName="likelihood")
# ss.marginal()
### Use path-sampling to calculate marginal likelihoods
# ps = pathSampler(file="outputPPsimple_cp/pow_p_DEC_simple_BF.out", powerColumnName="power", likelihoodColumnName="likelihood")
# ps.marginal()


# q()






